{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31f4ec48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\u\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fa783c6a1047f888805923069bc524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\u\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4196482c9442475581a5469b8546d9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9750ccd8afa43f398fbbb637288d7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60b746ff0224da6aef9b37d10b443f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f261c639464d01a0da09a4cef2f36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\u\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\u\\anaconda3\\Lib\\site-packages\\transformers\\trainer_tf.py:118: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification, TFTrainingArguments\n",
    "from transformers.trainer_tf import  TFTrainer\n",
    "\n",
    "# # Mount Google Drive and read the dataset\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "file_path = \"stress_data_updated.csv\"\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "# Convert labels from string to integer\n",
    "df[\"Type\"].replace({\"Stressed\": 0, \"Relaxed\": 1}, inplace=True)\n",
    "\n",
    "# Function to preprocess text data\n",
    "def preprocess(text):\n",
    "    if text is None:\n",
    "        return ''\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", text).lower()\n",
    "    return text\n",
    "\n",
    "df[\"clean_sentence\"] = df[\"Sentence\"].apply(preprocess)\n",
    "\n",
    "# Check for missing values and handle them\n",
    "df[\"clean_sentence\"].fillna(\"\", inplace=True)  # Replace missing values in Sentence with empty strings\n",
    "df[\"Type\"].fillna(0, inplace=True)  # Replace missing values in Type with 0\n",
    "\n",
    "# Convert labels to integer type\n",
    "df[\"Type\"] = df[\"Type\"].astype(int)\n",
    "\n",
    "# Separate the data into input features and labels\n",
    "X = df[\"clean_sentence\"]\n",
    "y = df[\"Type\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Initialize tokenizer and tokenize the input data\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, return_tensors=\"tf\")\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "# Convert encodings to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train.astype(np.int32)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test.astype(np.int32)))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=300,\n",
    "    weight_decay=0.0,  # Temporarily set weight decay to zero\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,  # Log at every 10 steps for debugging\n",
    "    evaluation_strategy=\"steps\",  # Set evaluation strategy to steps\n",
    "    eval_steps=100,  # Evaluate every 100 steps; adjust as necessary\n",
    ")\n",
    "\n",
    "# Initialize model and trainer\n",
    "with training_args.strategy.scope():\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25920f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8898ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       395\n",
      "           1       1.00      1.00      1.00       384\n",
      "\n",
      "    accuracy                           1.00       779\n",
      "   macro avg       1.00      1.00      1.00       779\n",
      "weighted avg       1.00      1.00      1.00       779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)\n",
    "pred=trainer.predict(test_dataset)[1]\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf268bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Save the model\n",
    "# model.save_pretrained({PATH})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66f41d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "import tensorflow as tf\n",
    "\n",
    "def predict_stress_level(custom_text, model_path=\"bert_stress_classifier\"):\n",
    "    # Load the saved model\n",
    "    loaded_model = TFDistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    # Tokenize the custom text\n",
    "    encoded_input = tokenizer(custom_text, truncation=True, padding=True, return_tensors='tf')\n",
    "\n",
    "    # Generate predictions\n",
    "    output = loaded_model(encoded_input)\n",
    "    predicted_class = tf.argmax(output.logits, axis=1).numpy()[0]\n",
    "\n",
    "    # Map predicted class to label\n",
    "    predicted_label = \"Stressed\" if predicted_class == 0 else \"Relaxed\"\n",
    "\n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "731598e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert_stress_classifier were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at bert_stress_classifier and are newly initialized: ['dropout_79']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Relaxed'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_stress_level(\"\"\"As I sit quietly in my favorite spot, listening to the gentle rustle of leaves and feeling the warmth of the sun on my skin,\n",
    "I can't help but feel a sense of calm wash over me, \n",
    "easing away the tensions of the day.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54aeda5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert_stress_classifier were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at bert_stress_classifier and are newly initialized: ['dropout_99']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Stressed'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_stress_level(\"\"\"\n",
    "As deadlines loom and tasks pile up, I feel the weight of responsibility bearing down on me,\n",
    "my heart racing with anxiety\n",
    "as I struggle to keep up with the demands of work and life\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ceaf2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
